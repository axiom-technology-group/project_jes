{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChinaDaily_WebData(part):\n",
    "\n",
    "    link_list = []\n",
    "    name_list = []\n",
    "    time_list = []\n",
    "    my_url = \"http://www.chinadaily.com.cn/\"+ part\n",
    "    uClient = uReq(my_url)\n",
    "    page_html = uClient.read()\n",
    "    uClient.close()\n",
    "\n",
    "    # doing the html parsing\n",
    "    page_soap = soup(page_html, \"html.parser\")\n",
    "    \n",
    "    # get the number of the pages\n",
    "    try:\n",
    "        pages = page_soap.findAll(\"div\",{\"selectpage res-m\"})\n",
    "        page = pages[0].span.a.text\n",
    "        page_num = page[21:]\n",
    "        \n",
    "    except:\n",
    "        pages = page_soap.findAll(\"div\",{\"selectpage\"})\n",
    "        page = pages[0].span.a.text\n",
    "        page_num = page[21:]\n",
    "        \n",
    "        \n",
    "       \n",
    "\n",
    "    #I have to count mannully for the number of the page\n",
    "    for i in range(1,int(page_num)+1):\n",
    "        my_url = \"http://www.chinadaily.com.cn/\"+ part + \"page_\" +str(i) + \".html\"\n",
    "        uClient = uReq(my_url)\n",
    "        page_html = uClient.read()\n",
    "        uClient.close()\n",
    "        # doing the html parsing\n",
    "        page_soap = soup(page_html, \"html.parser\")\n",
    "        #grab each product\n",
    "        containers = page_soap.findAll(\"div\",{\"mb10 tw3_01_2 \"})\n",
    "\n",
    "        for container in containers:\n",
    "            link_to_article = container.h4.a[\"href\"][2:]\n",
    "            link_list.append(link_to_article)\n",
    "            name_of_article = container.h4.a.text\n",
    "            name_list.append(name_of_article)\n",
    "            time_of_article = container.b.text\n",
    "            time_list.append(time_of_article)\n",
    "\n",
    "    author_list = [] \n",
    "    content_list = []\n",
    "    for link in link_list:\n",
    "\n",
    "        my_url = \"http://\" + link\n",
    "        uClient = uReq(my_url)\n",
    "        page_html = uClient.read()\n",
    "        uClient.close()\n",
    "        # doing the html parsing\n",
    "        page_soap = soup(page_html, \"html.parser\")\n",
    "        #get the authors name\n",
    "        info = page_soap.find(\"div\",{\"info\"})\n",
    "        try:\n",
    "            info_list = info.text.split(\"|\")\n",
    "            author = info_list[0]\n",
    "            author_list.append(author)\n",
    "        except:\n",
    "            author_list.append(np.nan)\n",
    "\n",
    "\n",
    "        try:\n",
    "            #if there is a parsing for filping pages\n",
    "            num_content = page_soap.find(\"span\",{\"pageno\"})\n",
    "            #get the number of pages\n",
    "            num=num_content.a.text.split(\"\\n\")[1].split(\"/\")[-1]\n",
    "\n",
    "\n",
    "            #get the content of the webpage\n",
    "            words = \"\"\n",
    "            # going through each pages of the article\n",
    "            for i in range(1,int(num)+1):\n",
    "\n",
    "                my_url = \"http://\" + link[:-5] +\"_\"+str(i)+\".html\"\n",
    "\n",
    "\n",
    "                uClient = uReq(my_url)\n",
    "                page_html = uClient.read()\n",
    "                uClient.close()\n",
    "                # doing the html parsing\n",
    "                page_soap = soup(page_html, \"html.parser\")\n",
    "\n",
    "                content = page_soap.findAll(\"p\",)\n",
    "                content = content[:-1]\n",
    "                for item in content:\n",
    "                    words += item.text\n",
    "\n",
    "\n",
    "\n",
    "        except:\n",
    "            words = \"\"\n",
    "            content = page_soap.findAll(\"p\",)\n",
    "            content = content[:-1]\n",
    "            for item in content:\n",
    "                words += item.text\n",
    "\n",
    "\n",
    "\n",
    "        #pend the if the content has a next page\n",
    "        #continue_run = page_soap.find(\"span\",{\"next\"})\n",
    "\n",
    "\n",
    "        content_list.append(words)\n",
    "\n",
    "    df = pd.DataFrame({'title of the article': name_list,\n",
    "                            'link of the article': link_list,\n",
    "                            'author': author_list,\n",
    "                            'content': content_list,                           \n",
    "                            'time of the article': time_list})\n",
    "    return df\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this function can scrap any part of the data from china daily\n",
    "## just select the articles from which section.\n",
    "## the example down below\n",
    "## the result will return a pandas dataframe with the requested section of the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "ChinaDaily_WebData(\"/sports/golf/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
